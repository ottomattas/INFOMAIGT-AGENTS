Please keep the question in your answer.
 
Question 1: What is the general idea of policy iteration?
To find the optimal policy through minor iterations or improvements made to the policy Ï€ through computing v_Ï€, and then doing it again until optimum is reached.

Question 2: What is the purpose of using the Q-function instead of the state value function?
To find the optimal policy without employing a model (model-free) and by making actions specific.

Question 3: Consider the SARSA algorithm. What is given, what is sampled by the policy and what is sampled by the environment?
Given is the starting point, goal and available actions. Policy samples state-action pairs to learn the value function (on-policy) while the environment is only interacted with.

Question 4: Why does the greedy improvement not work for the policy iteration?
Maybe I misunderstood but I thought that greedy policy improvement converges to Q* and Ï€*.

Question 5: What is the problem of the eps-greedy policy improvement?
Eps-greedy is an on-policy algorithm which learns only about the policy it is following.

Question 6: What is the first idea presented in the lecture to overcome the 
problem with eps-greedy?
To use Q-learning (off-policy) with an eps-greedy policy.

Question 7: Given the true state value function q_\pi and the dynamics p, give a formula that 
computes the value function v_\pi.
v_Ï€=q_Ï€|p ... or did you mean the Bellman equations?

Question 8: Give the update rule used in the SARSA algorithm.
Q(s_t,a_t) <- Q(s_t,a_t) + ð›¼[r_t+1 + ð›¾Q(s_t+1,a_t+1)-Q(s_t,a_t)]
Or a derivation
Q(s,a) = (1-ð›¼)*Q(s,a)+ð›¼[r+ð›¾Q(s',a')`]

Question 9: What is the key idea of Q-learning?
Exploration! To learn the optimal policy in a model-free environment through Q function approximation.

Question 10: What is an MDP with only one state?
Temporal Difference learning.